# LLM Factory Pattern Guide

> **üéØ Purpose**: ‡∏Ñ‡∏π‡πà‡∏°‡∏∑‡∏≠‡∏Å‡∏≤‡∏£‡πÉ‡∏ä‡πâ‡∏á‡∏≤‡∏ô Multi-LLM Architecture ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö MiMiVibes  
> **üë§ Target**: Personal Reference & Quick Implementation  
> **üìÖ Updated**: January 2025

## üöÄ Quick Start - ‡∏Å‡∏≤‡∏£‡∏™‡∏•‡∏±‡∏ö LLM

### 1. ‡∏™‡∏•‡∏±‡∏ö Provider ‡∏ú‡πà‡∏≤‡∏ô Environment Variables

#### Development (.env.local)

```bash
# ‡∏™‡∏•‡∏±‡∏ö‡πÄ‡∏õ‡πá‡∏ô OpenAI (Default)
DEFAULT_AI_PROVIDER=openai
AI_PROVIDER_FALLBACK=gemini

# ‡∏™‡∏•‡∏±‡∏ö‡πÄ‡∏õ‡πá‡∏ô Gemini
DEFAULT_AI_PROVIDER=gemini
AI_PROVIDER_FALLBACK=openai

# ‡∏õ‡∏¥‡∏î Fallback (‡πÑ‡∏°‡πà‡πÅ‡∏ô‡∏∞‡∏ô‡∏≥)
# AI_PROVIDER_FALLBACK=
```

#### Production (Vercel)

```bash
# Vercel Dashboard ‚Üí Settings ‚Üí Environment Variables
DEFAULT_AI_PROVIDER=openai
AI_PROVIDER_FALLBACK=gemini
OPENAI_API_KEY=sk-...
GOOGLE_AI_API_KEY=AIza...
```

### 2. ‡∏ó‡∏î‡∏™‡∏≠‡∏ö‡∏Å‡∏≤‡∏£‡∏™‡∏•‡∏±‡∏ö

```typescript
// ‡πÉ‡∏ä‡πâ default provider
const reading = await createProviderWithPrompt(prompt).invoke(messages);

// ‡∏£‡∏∞‡∏ö‡∏∏ provider ‡πÄ‡∏â‡∏û‡∏≤‡∏∞
const openaiReading = await llmManager
  .createWithPrompt(prompt, "openai")
  .invoke(messages);
const geminiReading = await llmManager
  .createWithPrompt(prompt, "gemini")
  .invoke(messages);
```

---

## üìñ Architecture Overview

### ‡πÑ‡∏ü‡∏•‡πå‡∏™‡∏≥‡∏Ñ‡∏±‡∏ç

```
/src/lib/ai/
‚îú‚îÄ‚îÄ providers/
‚îÇ   ‚îú‚îÄ‚îÄ base.ts          # LLMProvider interface
‚îÇ   ‚îú‚îÄ‚îÄ openai.ts        # OpenAI implementation
‚îÇ   ‚îú‚îÄ‚îÄ gemini.ts        # Gemini implementation
‚îÇ   ‚îî‚îÄ‚îÄ factory.ts       # Provider factory
‚îú‚îÄ‚îÄ manager.ts           # LLM manager + fallback logic
‚îú‚îÄ‚îÄ config.ts            # Configuration & environment
‚îî‚îÄ‚îÄ index.ts             # Public API
```

### Flow ‡∏Å‡∏≤‡∏£‡∏ó‡∏≥‡∏á‡∏≤‡∏ô

```
User Request ‚Üí LLM Manager ‚Üí Provider Factory ‚Üí Specific Provider ‚Üí LangChain ‚Üí AI API
                     ‚Üì
             Fallback Logic (‡∏ñ‡πâ‡∏≤ primary ‡∏•‡πà‡∏°)
```

---

## üîß ‡∏Å‡∏≤‡∏£‡πÄ‡∏û‡∏¥‡πà‡∏° LLM Provider ‡πÉ‡∏´‡∏°‡πà

### Step 1: ‡∏™‡∏£‡πâ‡∏≤‡∏á Provider Class

#### ‡∏ï‡∏±‡∏ß‡∏≠‡∏¢‡πà‡∏≤‡∏á: Claude Provider

**File**: `/src/lib/ai/providers/claude.ts`

```typescript
import { ChatAnthropic } from "@langchain/anthropic";
import {
  LLMProvider,
  LLMProviderConfig,
  LLMMessage,
  LLMResponse,
} from "./base";

export class ClaudeProvider implements LLMProvider {
  public readonly name = "claude";
  public readonly model: string;
  private client: ChatAnthropic;

  constructor(config: LLMProviderConfig) {
    this.model = config.model;
    this.client = new ChatAnthropic({
      model: config.model,
      apiKey: config.apiKey,
      temperature: config.temperature,
      maxTokens: config.maxTokens,
    });
  }

  async invoke(messages: LLMMessage[]): Promise<LLMResponse> {
    const response = await this.client.invoke(messages);
    return {
      content: response.content as string,
      usage: response.usage_metadata
        ? {
            promptTokens: response.usage_metadata.input_tokens,
            completionTokens: response.usage_metadata.output_tokens,
            totalTokens: response.usage_metadata.total_tokens,
          }
        : undefined,
    };
  }

  createWithPrompt(systemPrompt: string) {
    return {
      invoke: async (messages: LLMMessage[]) => {
        const fullMessages = [
          { role: "system" as const, content: systemPrompt },
          ...messages,
        ];
        return await this.invoke(fullMessages);
      },
    };
  }
}
```

### Step 2: ‡πÄ‡∏û‡∏¥‡πà‡∏°‡πÉ‡∏ô Factory

**File**: `/src/lib/ai/providers/factory.ts`

```typescript
import { ClaudeProvider } from "./claude";

export type ProviderType = "openai" | "gemini" | "claude"; // ‡πÄ‡∏û‡∏¥‡πà‡∏° claude

export class ProviderFactory {
  static create(type: ProviderType, config: LLMProviderConfig): LLMProvider {
    switch (type) {
      case "openai":
        return new OpenAIProvider(config);
      case "gemini":
        return new GeminiProvider(config);
      case "claude":
        return new ClaudeProvider(config); // ‡πÄ‡∏û‡∏¥‡πà‡∏°‡∏ô‡∏µ‡πâ
      default:
        throw new Error(`Unsupported provider type: ${type}`);
    }
  }

  static getSupportedProviders(): ProviderType[] {
    return ["openai", "gemini", "claude"]; // ‡πÄ‡∏û‡∏¥‡πà‡∏° claude
  }
}
```

### Step 3: ‡πÄ‡∏û‡∏¥‡πà‡∏° Configuration

**File**: `/src/lib/ai/config.ts`

```typescript
export function getAIConfig(): AIConfig {
  return {
    defaultProvider,
    fallbackProvider,
    providers: {
      openai: {
        /* existing */
      },
      gemini: {
        /* existing */
      },
      claude: {
        name: "claude",
        model: process.env.CLAUDE_MODEL || "claude-3-sonnet-20240229",
        apiKey: process.env.ANTHROPIC_API_KEY || "",
        temperature: 0.7,
        maxTokens: 4096,
        enabled: !!process.env.ANTHROPIC_API_KEY,
      },
    },
  };
}
```

### Step 4: ‡πÄ‡∏û‡∏¥‡πà‡∏° Environment Variables

```bash
# .env.local
ANTHROPIC_API_KEY=sk-ant-...
CLAUDE_MODEL=claude-3-sonnet-20240229
DEFAULT_AI_PROVIDER=claude
```

### Step 5: Install Dependencies

```bash
npm install @langchain/anthropic
```

---

## üí∞ Cost & Performance Comparison

### ‡∏£‡∏≤‡∏Ñ‡∏≤‡∏õ‡∏£‡∏∞‡∏°‡∏≤‡∏ì (‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö Thai Tarot Reading ~1,500 tokens)

| Provider | Model       | Input Cost  | Output Cost | Total/Reading |
| -------- | ----------- | ----------- | ----------- | ------------- |
| OpenAI   | GPT-4-turbo | $0.01/1K    | $0.03/1K    | ~$0.06        |
| Gemini   | 2.0 Flash   | $0.00075/1K | $0.003/1K   | ~$0.005       |
| Claude   | 3.5 Sonnet  | $0.003/1K   | $0.015/1K   | ~$0.027       |

### Performance Characteristics

#### OpenAI GPT-4-turbo

- ‚úÖ **‡∏î‡∏µ**: Quality ‡∏™‡∏π‡∏á, ‡∏à‡∏±‡∏î‡∏Å‡∏≤‡∏£‡∏†‡∏≤‡∏©‡∏≤‡πÑ‡∏ó‡∏¢‡πÑ‡∏î‡πâ‡∏î‡∏µ, JSON format consistency
- ‚ö†Ô∏è **‡∏£‡∏∞‡∏ß‡∏±‡∏á**: ‡∏£‡∏≤‡∏Ñ‡∏≤‡πÅ‡∏û‡∏á‡∏™‡∏∏‡∏î, rate limit 5,000 RPM
- üéØ **‡πÄ‡∏´‡∏°‡∏≤‡∏∞‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö**: Production, quality-first scenarios

#### Gemini 2.0 Flash

- ‚úÖ **‡∏î‡∏µ**: ‡∏£‡∏≤‡∏Ñ‡∏≤‡∏ñ‡∏π‡∏Å‡∏™‡∏∏‡∏î, ‡πÄ‡∏£‡πá‡∏ß, ‡∏ü‡∏£‡∏µ tier ‡πÉ‡∏´‡∏ç‡πà
- ‚ö†Ô∏è **‡∏£‡∏∞‡∏ß‡∏±‡∏á**: Quality ‡∏ï‡πà‡∏≥‡∏Å‡∏ß‡πà‡∏≤ GPT-4, ‡∏ö‡∏≤‡∏á‡∏Ñ‡∏£‡∏±‡πâ‡∏á‡∏à‡∏±‡∏î‡∏Å‡∏≤‡∏£ JSON ‡πÑ‡∏°‡πà‡∏î‡∏µ
- üéØ **‡πÄ‡∏´‡∏°‡∏≤‡∏∞‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö**: Development, cost-sensitive scenarios, fallback

#### Claude 3.5 Sonnet

- ‚úÖ **‡∏î‡∏µ**: Quality ‡∏î‡∏µ, safety filters ‡∏î‡∏µ, ‡∏à‡∏±‡∏î‡∏Å‡∏≤‡∏£ context ‡∏¢‡∏≤‡∏ß‡πÑ‡∏î‡πâ
- ‚ö†Ô∏è **‡∏£‡∏∞‡∏ß‡∏±‡∏á**: ‡∏£‡∏≤‡∏Ñ‡∏≤‡∏õ‡∏≤‡∏ô‡∏Å‡∏•‡∏≤‡∏á, rate limit ‡πÄ‡∏Ç‡πâ‡∏°‡∏Å‡∏ß‡πà‡∏≤
- üéØ **‡πÄ‡∏´‡∏°‡∏≤‡∏∞‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö**: Alternative to GPT-4, content moderation

---

## ‚ö†Ô∏è ‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏£‡∏£‡∏∞‡∏ß‡∏±‡∏á & Best Practices

### 1. Rate Limiting

```typescript
// ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö rate limit ‡∏Ç‡∏≠‡∏á‡πÅ‡∏ï‡πà‡∏•‡∏∞ provider
const RATE_LIMITS = {
  openai: { rpm: 5000, tpm: 800000 },
  gemini: { rpm: 1500, tpm: 32000 }, // Free tier
  claude: { rpm: 1000, tpm: 40000 },
};
```

### 2. Error Handling

```typescript
// ‡∏ï‡∏±‡∏ß‡∏≠‡∏¢‡πà‡∏≤‡∏á fallback logic
try {
  return await primaryProvider.invoke(messages);
} catch (error) {
  console.warn(`Primary provider failed: ${error.message}`);
  if (fallbackProvider) {
    return await fallbackProvider.invoke(messages);
  }
  throw error;
}
```

### 3. Response Format Differences

#### OpenAI Response

```json
{
  "content": "...",
  "usage": {
    "prompt_tokens": 150,
    "completion_tokens": 1200,
    "total_tokens": 1350
  }
}
```

#### Gemini Response

```json
{
  "content": "...",
  "usage_metadata": {
    "input_tokens": 150,
    "output_tokens": 1200,
    "total_tokens": 1350
  }
}
```

### 4. JSON Parsing Considerations

```typescript
// Provider-specific JSON handling
const PROVIDER_QUIRKS = {
  gemini: {
    // ‡∏°‡∏±‡∏Å‡∏à‡∏∞‡πÉ‡∏™‡πà markdown code blocks
    needsMarkdownStripping: true,
    // ‡∏ö‡∏≤‡∏á‡∏Ñ‡∏£‡∏±‡πâ‡∏á JSON ‡πÑ‡∏°‡πà complete
    needsTruncationFix: true,
  },
  openai: {
    // JSON format ‡∏Ñ‡πà‡∏≠‡∏ô‡∏Ç‡πâ‡∏≤‡∏á consistent
    needsMarkdownStripping: false,
    needsTruncationFix: false,
  },
};
```

### 5. Environment Security

```bash
# ‚ö†Ô∏è ‡∏≠‡∏¢‡πà‡∏≤‡πÉ‡∏™‡πà API keys ‡πÉ‡∏ô git
# ‚úÖ ‡πÉ‡∏ä‡πâ .env.local ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö development
# ‚úÖ ‡πÉ‡∏ä‡πâ Vercel dashboard ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö production

# Example .env.local
OPENAI_API_KEY=sk-...
GOOGLE_AI_API_KEY=AIza...
ANTHROPIC_API_KEY=sk-ant-...
```

---

## üõ†Ô∏è Debugging & Monitoring

### 1. Provider Status Check

```typescript
// ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö providers ‡∏ó‡∏µ‡πà‡πÉ‡∏ä‡πâ‡πÑ‡∏î‡πâ
const availableProviders = llmManager.getAvailableProviders();
console.log("Available providers:", availableProviders);

// ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö default provider
const defaultProvider = llmManager.getProvider();
console.log("Current default:", defaultProvider.name);
```

### 2. Token Usage Monitoring

```typescript
// ‡∏ï‡∏¥‡∏î‡∏ï‡∏≤‡∏° token usage
const response = await provider.invoke(messages);
if (response.usage) {
  console.log(`Tokens used: ${response.usage.totalTokens}`);
  console.log(`Cost estimate: $${response.usage.totalTokens * 0.00003}`);
}
```

### 3. Performance Testing

```typescript
// ‡∏ó‡∏î‡∏™‡∏≠‡∏ö response time
const start = Date.now();
const response = await provider.invoke(messages);
const duration = Date.now() - start;
console.log(`Response time: ${duration}ms`);
```

---

## üöÄ Use Cases & Scenarios

### Scenario 1: Development Testing

```bash
# ‡πÉ‡∏ä‡πâ Gemini ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö development (‡∏£‡∏≤‡∏Ñ‡∏≤‡∏ñ‡∏π‡∏Å)
DEFAULT_AI_PROVIDER=gemini
AI_PROVIDER_FALLBACK=openai
```

### Scenario 2: Production Quality

```bash
# ‡πÉ‡∏ä‡πâ OpenAI ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö production (quality ‡∏™‡∏π‡∏á)
DEFAULT_AI_PROVIDER=openai
AI_PROVIDER_FALLBACK=gemini
```

### Scenario 3: Cost Optimization

```bash
# ‡πÉ‡∏ä‡πâ Gemini ‡∏´‡∏•‡∏±‡∏Å, OpenAI fallback
DEFAULT_AI_PROVIDER=gemini
AI_PROVIDER_FALLBACK=openai
```

### Scenario 4: A/B Testing

```typescript
// ‡∏™‡∏∏‡πà‡∏°‡πÄ‡∏•‡∏∑‡∏≠‡∏Å provider ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö A/B test
const provider = Math.random() > 0.5 ? "openai" : "gemini";
const response = await llmManager
  .createWithPrompt(prompt, provider)
  .invoke(messages);
```

---

## üìö Quick Reference

### Environment Variables

```bash
# Provider Selection
DEFAULT_AI_PROVIDER=openai|gemini|claude
AI_PROVIDER_FALLBACK=openai|gemini|claude

# API Keys
OPENAI_API_KEY=sk-...
GOOGLE_AI_API_KEY=AIza...
ANTHROPIC_API_KEY=sk-ant-...

# Model Selection (Optional)
OPENAI_MODEL=gpt-4-turbo
GEMINI_MODEL=gemini-2.0-flash-exp
CLAUDE_MODEL=claude-3-sonnet-20240229
```

### Code Snippets

```typescript
// Import
import { llmManager, createProviderWithPrompt } from "@/lib/ai";

// Use default provider
const defaultResponse = await createProviderWithPrompt(prompt).invoke(messages);

// Use specific provider
const openaiResponse = await llmManager
  .createWithPrompt(prompt, "openai")
  .invoke(messages);

// Check available providers
const providers = llmManager.getAvailableProviders();
```

---

**üí° Tips**:

- ‡πÄ‡∏£‡∏¥‡πà‡∏°‡∏î‡πâ‡∏ß‡∏¢ Gemini ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö development
- ‡πÄ‡∏õ‡∏•‡∏µ‡πà‡∏¢‡∏ô‡πÄ‡∏õ‡πá‡∏ô OpenAI ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö production
- ‡πÉ‡∏ä‡πâ fallback ‡πÄ‡∏™‡∏°‡∏≠‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°‡∏ô‡πà‡∏≤‡πÄ‡∏ä‡∏∑‡πà‡∏≠‡∏ñ‡∏∑‡∏≠
- Monitor token usage ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏Ñ‡∏ß‡∏ö‡∏Ñ‡∏∏‡∏°‡∏Ñ‡πà‡∏≤‡πÉ‡∏ä‡πâ‡∏à‡πà‡∏≤‡∏¢

**‚ö†Ô∏è Warning**:

- ‡∏≠‡∏¢‡πà‡∏≤‡πÉ‡∏™‡πà API keys ‡πÉ‡∏ô code
- ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö rate limits ‡∏Å‡πà‡∏≠‡∏ô‡πÉ‡∏ä‡πâ‡πÉ‡∏ô‡∏Å‡∏≤‡∏£‡∏ú‡∏•‡∏¥‡∏ï
- Test providers ‡πÉ‡∏´‡∏°‡πà‡∏Å‡πà‡∏≠‡∏ô‡πÉ‡∏ä‡πâ‡πÉ‡∏ô production
